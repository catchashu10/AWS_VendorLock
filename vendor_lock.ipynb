{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Dataset Preparation\n",
    "Use the graph dataset and load it in readable format in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                           services  \\\n",
      "0  WS2Qgx0qgCM       [EC2, EC2, CloudTrail, OpenSearch, STS, IAM]   \n",
      "1  AzM_d7ZvzUE  [Lambda, Lambda, Lambda, Kinesis, Kinesis, Aur...   \n",
      "2  6LcSv9XocTY  [UserConsumerWeb, Connect, S3, S3, Transcribe,...   \n",
      "3  3yJZ6rPoZfg    [EKS, EKS, VPC, SQS, EC2, S3, UserConsumerEdge]   \n",
      "4  JRDGId6N49E  [ThirdParty, EC2, S3, S3, CloudFormation, Serv...   \n",
      "\n",
      "                                               edges  \\\n",
      "0  [{'source': 'EC2', 'target': 'EC2', 'type': 'd...   \n",
      "1  [{'source': 'Lambda', 'target': 'Kinesis', 'ty...   \n",
      "2  [{'source': 'UserConsumerWeb', 'target': 'Conn...   \n",
      "3  [{'source': 'EKS', 'target': 'S3', 'type': 'da...   \n",
      "4  [{'source': 'ThirdParty', 'target': 'EC2', 'ty...   \n",
      "\n",
      "                                                name  \\\n",
      "0  Airbnb: Securing MultiTenant Kubernetes Cluste...   \n",
      "1  Epsagon: Automatically Tracing and Analyzing B...   \n",
      "2  Intuit: Serving 7 Million Customers Using Amaz...   \n",
      "3  Hexagon HxDR: Cloud-Based Visualization of Spa...   \n",
      "4  TRI-AD: Large-Scale and High-Performance Distr...   \n",
      "\n",
      "                                          link  \\\n",
      "0  https://www.youtube.com/watch?v=WS2Qgx0qgCM   \n",
      "1  https://www.youtube.com/watch?v=AzM_d7ZvzUE   \n",
      "2  https://www.youtube.com/watch?v=6LcSv9XocTY   \n",
      "3  https://www.youtube.com/watch?v=3yJZ6rPoZfg   \n",
      "4  https://www.youtube.com/watch?v=JRDGId6N49E   \n",
      "\n",
      "                          categories  \\\n",
      "0                     internal_infra   \n",
      "1                        data_ingest   \n",
      "2  backend_query_handler,data_ingest   \n",
      "3                  compute_framework   \n",
      "4                  compute_framework   \n",
      "\n",
      "                                               notes  \n",
      "0  Airbnb runs their own Kubernetes clusters on E...  \n",
      "1  Monitoring company100s of 1000s of Lambdas are...  \n",
      "2  Transcribe and Comprehend - easy to use, cost ...  \n",
      "3  - HxDR is a cloud-based visualization platform...  \n",
      "4  autonomous driving. safe cars. high compute an...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "# Path to extracted files\n",
    "graphml_dir = 'input/exported_aws_graphs'\n",
    "\n",
    "# List all graph files\n",
    "graph_files = [os.path.join(graphml_dir, f) for f in os.listdir(graphml_dir) if f.endswith('.graphml')]\n",
    "\n",
    "# Initialize data storage\n",
    "architectures = []\n",
    "\n",
    "################################ For debugging and checking Graph Data #################################\n",
    "\n",
    "# Inspect node attributes dynamically\n",
    "# for graph_file in graph_files[:1]:  # Check just one file for debugging\n",
    "#     graph = nx.read_graphml(graph_file)\n",
    "#     for node in graph.nodes:\n",
    "#         print(f\"Node ID: {node}, Attributes: {graph.nodes[node]}\")\n",
    "\n",
    "# Inspect edge attributes dynamically\n",
    "# for graph_file in graph_files[:1]:  # Check just one file for debugging\n",
    "#     graph = nx.read_graphml(graph_file)\n",
    "#     for edge in graph.edges(data=True):\n",
    "#         print(f\"Edge: {edge}\")\n",
    "\n",
    "# Inspect graph attributes dynamically\n",
    "# for graph_file in graph_files[:1]:  # Check just one file for debugging\n",
    "#     graph = nx.read_graphml(graph_file)\n",
    "#     print(\"Graph Metadata:\", graph.graph)\n",
    "\n",
    "##########################################################################################################\n",
    "\n",
    "# Process each graph file\n",
    "for graph_file in graph_files:\n",
    "    graph = nx.read_graphml(graph_file)\n",
    "    architecture_id = os.path.basename(graph_file).split('.')[0]\n",
    "\n",
    "    # Extract node details\n",
    "    nodes = {\n",
    "        node: {\n",
    "            \"service\": graph.nodes[node].get('service', 'NULL'),\n",
    "            \"human_name\": graph.nodes[node].get('human_name', 'NULL'),\n",
    "            \"info\": graph.nodes[node].get('info', '')\n",
    "        }\n",
    "        for node in graph.nodes\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Map nodes to services\n",
    "    services = [data['service'] for data in nodes.values()]\n",
    "    \n",
    "    # Extract edges with meaningful details\n",
    "    edges = [\n",
    "        {\n",
    "            \"source\": nodes[src][\"service\"],\n",
    "            \"target\": nodes[dst][\"service\"],\n",
    "            \"type\": edge_data.get('type', 'NULL')\n",
    "        }\n",
    "        for src, dst, edge_data in graph.edges(data=True)  # Unpack the edge attributes\n",
    "    ]\n",
    "    \n",
    "    # Add processed architecture\n",
    "    architectures.append({\n",
    "        \"id\": architecture_id,\n",
    "        \"services\": services,\n",
    "        \"edges\": edges,\n",
    "        \"name\": graph.graph.get('name', 'Unnamed Architecture'),\n",
    "        \"link\": graph.graph.get('link', ''),\n",
    "        \"categories\": graph.graph.get('categories', ''),\n",
    "        \"notes\": graph.graph.get('notes', '')\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "architecture_df = pd.DataFrame(architectures)\n",
    "\n",
    "# Display the processed data\n",
    "print(architecture_df.head())\n",
    "\n",
    "# Save to CSV for further analysis\n",
    "architecture_df.to_csv('output/architectures.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2: Find all the unique AWS services among all the architectures\n",
    "    - Create a list of all the unique services as all_services and keep appending all the unique services to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_services = []\n",
    "for i in range(len(architectures)):\n",
    "    for service in architectures[i]['services']:\n",
    "        if service not in all_services:\n",
    "            all_services.append(service)\n",
    "\n",
    "# Remove entries which contains \"User\" or \"ThirdParty\" in all_services\n",
    "# This is done to remove the user created services from the list\n",
    "all_services = [service for service in all_services if \"User\" not in service]\n",
    "all_services = [service for service in all_services if \"ThirdParty\" not in service]\n",
    "\n",
    "all_services = sorted(all_services)\n",
    "\n",
    "# Save all_services to a csv file\n",
    "all_services_df = pd.DataFrame(all_services)\n",
    "all_services_df.to_csv('output/aws_services.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of services (that are not AWS services) to be removed\n",
    "extra_services = {'CouchBase', 'MongoDBAtlas', 'OnPremDC', 'SAP', 'ServiceNow'}\n",
    "\n",
    "# Remove the extra services\n",
    "all_services = [service for service in all_services if service not in extra_services]\n",
    "\n",
    "# Save the cleaned list of services to a CSV file\n",
    "all_services_df = pd.DataFrame(all_services)\n",
    "all_services_df.to_csv('output/aws_services.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                               name  \\\n",
      "0  QnwfcDZkwh8  Mediaset: Achieving an Omni-Channel Broadcasti...   \n",
      "1  -3lnf5lzsH0           MakeMyTrip: Building Next Generation SOC   \n",
      "2  gD8pzUnXgsU  Liberty Mutual: Moving Towards RealTime Financ...   \n",
      "3  CsD5bmM6mpY  Linke - a Syntax Company: Building a Business ...   \n",
      "4  HcmEFZukA-Y  7-Eleven: Innovation with Serverless for Cash-...   \n",
      "5  9LhiUsg3knw  Musixmatch: How Content is Generated by Users ...   \n",
      "6  wtl7CrSQnHA  Fresenius Medical Care: Enabling Patient Care ...   \n",
      "7  AS2JeM2FUzE  Intellect Design Arena: Insurance Risk Assessm...   \n",
      "8  XmIhHtPJWog  HDI Group: Automated Security and Compliance I...   \n",
      "9  iwZLadQ3XpY  Riot Games: A new data ingest pipeline for lea...   \n",
      "\n",
      "   extra_services  \n",
      "0      [OnPremDC]  \n",
      "1      [OnPremDC]  \n",
      "2      [OnPremDC]  \n",
      "3           [SAP]  \n",
      "4  [MongoDBAtlas]  \n",
      "5     [CouchBase]  \n",
      "6      [OnPremDC]  \n",
      "7  [MongoDBAtlas]  \n",
      "8    [ServiceNow]  \n",
      "9      [OnPremDC]  \n"
     ]
    }
   ],
   "source": [
    "architectures_with_extra_services = []\n",
    "\n",
    "# Process each graph file\n",
    "for graph_file in graph_files:\n",
    "    graph = nx.read_graphml(graph_file)\n",
    "    architecture_id = os.path.basename(graph_file).split('.')[0]\n",
    "\n",
    "    # Extract node details\n",
    "    nodes = {\n",
    "        node: {\n",
    "            \"service\": graph.nodes[node].get('service', 'NULL'),\n",
    "            \"human_name\": graph.nodes[node].get('human_name', 'NULL'),\n",
    "            \"info\": graph.nodes[node].get('info', '')\n",
    "        }\n",
    "        for node in graph.nodes\n",
    "    }\n",
    "\n",
    "    # Map nodes to services\n",
    "    services = [data['service'] for data in nodes.values()]\n",
    "    \n",
    "    # Check if any extra service is present in this architecture\n",
    "    if any(service in extra_services for service in services):\n",
    "        # Add architecture information to the list\n",
    "        architectures_with_extra_services.append({\n",
    "            \"id\": architecture_id,\n",
    "            \"name\": graph.graph.get('name', 'Unnamed Architecture'),\n",
    "            \"extra_services\": [service for service in services if service in extra_services]\n",
    "        })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "arch_df = pd.DataFrame(architectures_with_extra_services)\n",
    "\n",
    "# Print the DataFrame with architectures containing extra services\n",
    "print(arch_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Count of each unique AWS service across all the cloud architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Service  Usage Count  Percentage\n",
      "106                    S3          253       63.89\n",
      "75                 Lambda          220       55.56\n",
      "44                    EC2          151       38.13\n",
      "41               DynamoDB          123       31.06\n",
      "9              ApiGateway           96       24.24\n",
      "..                    ...          ...         ...\n",
      "60                Grafana            1        0.25\n",
      "53          ElementalLive            1        0.25\n",
      "112  SageMakerGroundTruth            1        0.25\n",
      "88          ModelRegistry            1        0.25\n",
      "0                     ACM            1        0.25\n",
      "\n",
      "[134 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store the usage count for each service\n",
    "service_usage_count = {service: 0 for service in all_services}\n",
    "\n",
    "# Calculate the usage count of each service across all architectures\n",
    "for architecture in architectures:\n",
    "    for service in set(architecture['services']):\n",
    "        if service in service_usage_count:\n",
    "            service_usage_count[service] += 1\n",
    "\n",
    "# Convert the usage count dictionary to a DataFrame\n",
    "service_usage_df = pd.DataFrame(\n",
    "    list(service_usage_count.items()), \n",
    "    columns=[\"Service\", \"Usage Count\"]\n",
    ")\n",
    "\n",
    "# Calculate the percentage of architectures for each service\n",
    "service_usage_df[\"Percentage\"] = (\n",
    "    service_usage_df[\"Usage Count\"] / len(architectures) * 100\n",
    ").round(2)  # Round to 2 decimal places for clarity\n",
    "\n",
    "# Sort the DataFrame by usage count in descending order\n",
    "service_usage_df = service_usage_df.sort_values(by=\"Usage Count\", ascending=False)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "service_usage_df.to_csv('output/aws_service_usage.csv', index=False)\n",
    "\n",
    "# Display the top entries of the DataFrame\n",
    "print(service_usage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "target_services = ['WAF', 'AWSConfig', 'Neptune', 'RoboMaker', 'StorageGateway', 'Inspector', 'Timestream', 'SES']\n",
    "\n",
    "# Calculate the total number of architectures affected by the target services\n",
    "affected_architectures = set()\n",
    "\n",
    "for architecture in architectures:\n",
    "    # Check if any of the target services are in the current architecture\n",
    "    if any(service in target_services for service in architecture['services']):\n",
    "        affected_architectures.add(architecture['id'])\n",
    "\n",
    "affected_architectures_df = pd.DataFrame(affected_architectures)\n",
    "print(len(affected_architectures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures with S3 and Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             id                                               name  \\\n",
      "48  dy-drIboyNA  Healthdirect Australia: Using AWS to Connect P...   \n",
      "60  hFx0EF9KEZU  Buzzvil: Segment Processing and Provision for ...   \n",
      "51  PoYiSKUy8sE  Alert Logic: Scaling Storage and Delivery of P...   \n",
      "62  l0hlxVNmJPI  eCloudvalley: Build Up An EnterpriseLevel IoT ...   \n",
      "7   F4KDOGNpSoI  Pernod Ricard: Shortening URLs on a Global Sca...   \n",
      "\n",
      "                                             services  service_count  \n",
      "48  [UserConsumerAPI, UserConsumerAPI, S3, S3, Api...             15  \n",
      "60  [UserConsumerWebMobile, EKS, EKS, ThirdParty, ...             15  \n",
      "51  [SQS, StepFunctions, ECS, ECS, Lambda, Lambda,...             14  \n",
      "62  [UserConsumerMobile, UserConsumerEdge, IoTCore...             14  \n",
      "7   [CloudFront, LambdaAtEdge, DynamoDB, S3, S3, L...             14  \n"
     ]
    }
   ],
   "source": [
    "# List to store architectures containing both 'S3' and 'Lambda'\n",
    "architectures_with_s3_lambda = []\n",
    "\n",
    "for _, row in architecture_df.iterrows():\n",
    "    # Check if both 'S3' and 'Lambda' are in the current architecture's services\n",
    "    if 'S3' in row['services'] and 'Lambda' in row['services'] and 'DynamoDB' in row['services']:\n",
    "        architectures_with_s3_lambda.append({\n",
    "            \"id\": row['id'],\n",
    "            \"name\": row['name'],\n",
    "            \"services\": row['services']\n",
    "        })\n",
    "\n",
    "# Convert the architectures list to a DataFrame\n",
    "architectures_with_s3_lambda_df = pd.DataFrame(architectures_with_s3_lambda)\n",
    "\n",
    "# Add a column for the number of services in each architecture\n",
    "architectures_with_s3_lambda_df['service_count'] = architectures_with_s3_lambda_df['services'].apply(len)\n",
    "\n",
    "# Sort the DataFrame by the service count in descending order\n",
    "architectures_with_s3_lambda_df = architectures_with_s3_lambda_df.sort_values(by='service_count', ascending=False)\n",
    "\n",
    "# Display the sorted architectures with 'S3' and 'Lambda'\n",
    "print(architectures_with_s3_lambda_df.head())\n",
    "\n",
    "# Save to CSV for further analysis\n",
    "architectures_with_s3_lambda_df.to_csv('output/architectures_with_s3_lambda.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3: Load the compatibitliy matrix from the JSON file\n",
    "    - Mapping AWS services to Azure and GCP counterparts\n",
    "    - This needs to be researched and created manually\n",
    "    - One good reference is : https://github.com/milanm/Cloud-Product-Mapping\n",
    "\n",
    "# Example of service_mapping:\n",
    "```python\n",
    "service_mapping = {\n",
    "    \"S3\": {\"Azure\": \"Blob Storage\", \"GCP\": \"Cloud Storage\", \"Compatibility\": \"Fully Compatible\"},\n",
    "    \"Lambda\": {\"Azure\": \"Functions\", \"GCP\": \"Cloud Functions\", \"Compatibility\": \"Fully Compatible\"},\n",
    "    \"Athena\": {\"Azure\": None, \"GCP\": None, \"Compatibility\": \"Incompatible\"},\n",
    "    \"DynamoDB\": {\"Azure\": \"Cosmos DB\", \"GCP\": \"Firestore\", \"Compatibility\": \"Partially Compatible\"},\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Azure                    GCP  \\\n",
      "AWS                                                                   \n",
      "ACM                 App Service Certificates    Certificate Manager   \n",
      "ALB                      Application Gateway   Cloud Load Balancing   \n",
      "AMI             Azure Virtual Machine Images          Custom Images   \n",
      "AWSConfig                       Azure Policy  Cloud Asset Inventory   \n",
      "AccessAnalyzer                 Azure Monitor       Cloud Audit Logs   \n",
      "...                                      ...                    ...   \n",
      "VPCPeering           Virtual Network Peering            VPC Peering   \n",
      "VPN                              VPN Gateway              Cloud VPN   \n",
      "WAF                 Web Application Firewall            Cloud Armor   \n",
      "WorkSpaces             Azure Virtual Desktop                   None   \n",
      "XRay                    Application Insights            Cloud Trace   \n",
      "\n",
      "                       Compatibility  \n",
      "AWS                                   \n",
      "ACM                 Fully Compatible  \n",
      "ALB                 Fully Compatible  \n",
      "AMI                 Fully Compatible  \n",
      "AWSConfig       Partially Compatible  \n",
      "AccessAnalyzer  Partially Compatible  \n",
      "...                              ...  \n",
      "VPCPeering          Fully Compatible  \n",
      "VPN                 Fully Compatible  \n",
      "WAF                 Fully Compatible  \n",
      "WorkSpaces      Partially Compatible  \n",
      "XRay                Fully Compatible  \n",
      "\n",
      "[139 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('input/compatibility_matrix_2.json', 'r') as file:\n",
    "    service_mapping = json.load(file)\n",
    "\n",
    "# Convert mapping to DataFrame\n",
    "mapping_df = pd.DataFrame.from_dict(service_mapping, orient='index')\n",
    "mapping_df.index.name = \"AWS\"\n",
    "print(mapping_df)\n",
    "\n",
    "# Save mapping for later reference\n",
    "mapping_df.to_csv('output/service_mapping.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4: Compute the lock-in index for all the architectures\n",
    "It computes how many services in each architectures are incompatible or partially compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id  lock_in_index  lock_in_category\n",
      "0    WS2Qgx0qgCM       0.083333      High Lock-In\n",
      "1    AzM_d7ZvzUE       0.000000       Low Lock-In\n",
      "2    6LcSv9XocTY       0.045455  Moderate Lock-In\n",
      "3    3yJZ6rPoZfg       0.000000       Low Lock-In\n",
      "4    JRDGId6N49E       0.000000       Low Lock-In\n",
      "..           ...            ...               ...\n",
      "391  ItpY_KKR94k       0.062500  Moderate Lock-In\n",
      "392  NfUwtK8ALtw       0.000000       Low Lock-In\n",
      "393  0wnNlOg42dc       0.000000       Low Lock-In\n",
      "394  chQ1phTqvnY       0.000000       Low Lock-In\n",
      "395  aOZ4H98XROc       0.000000       Low Lock-In\n",
      "\n",
      "[396 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calcualte Lock-In Index\n",
    "# Define weights\n",
    "weights = {\n",
    "    'Fully Compatible': 0,\n",
    "    'Partially Compatible': 0.5,\n",
    "    'Incompatible': 1\n",
    "}\n",
    "\n",
    "# Merge architectures with service mapping\n",
    "# def compute_lock_in_index(services):\n",
    "#     proprietary_count = sum(1 for s in services if service_mapping.get(s, {}).get(\"Compatibility\") != \"Fully Compatible\")\n",
    "#     return proprietary_count / len(services) if services else 0\n",
    "\n",
    "\n",
    "# Function to calculate Lock-In Index\n",
    "def compute_lock_in_index(services):\n",
    "    total_weight = 0\n",
    "    for service in services:\n",
    "        compatibility = service_mapping.get(service, {}).get('Compatibility', '')\n",
    "        total_weight += weights.get(compatibility, 0)\n",
    "    return total_weight / len(services) if services else 0\n",
    "\n",
    "# # Add Lock-In Index for each architecture\n",
    "architecture_df['lock_in_index'] = architecture_df['services'].apply(compute_lock_in_index)\n",
    "\n",
    "# Categorize architectures by lock-in level\n",
    "def categorize_lock_in(index):\n",
    "    if index >= 0.07:\n",
    "        return \"High Lock-In\"\n",
    "    elif index >= 0.04:\n",
    "        return \"Moderate Lock-In\"\n",
    "    else:\n",
    "        return \"Low Lock-In\"\n",
    "\n",
    "architecture_df['lock_in_category'] = architecture_df['lock_in_index'].apply(categorize_lock_in)\n",
    "print(architecture_df[['id', 'lock_in_index', 'lock_in_category']])\n",
    "\n",
    "# Save updated dataset\n",
    "architecture_df.to_csv('output/lock_in_analysis.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step5: Further Analysis:\n",
    "    - Aanalyze which services contributes most to the lock-in in different architectures.\n",
    "    - Assess migration feasibility\n",
    "    - Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical Proprietary Services:\n",
      "OpenSearch           39\n",
      "LambdaAtEdge         11\n",
      "AppSync              10\n",
      "ControlTower          8\n",
      "Connect               7\n",
      "AWSConfig             7\n",
      "Neptune               7\n",
      "SystemsManager        6\n",
      "MediaPackage          4\n",
      "SES                   4\n",
      "MediaLive             3\n",
      "Chime                 2\n",
      "GlobalAccelerator     2\n",
      "ElementalLive         2\n",
      "WorkSpaces            2\n",
      "StorageGateway        2\n",
      "AlexaForBusiness      2\n",
      "Timestream            2\n",
      "MediaConnect          2\n",
      "RoboMaker             1\n",
      "MAM                   1\n",
      "CouchBase             1\n",
      "AccessAnalyzer        1\n",
      "DeepLens              1\n",
      "DataExchange          1\n",
      "Grafana               1\n",
      "KinesisVideo          1\n",
      "AppStream             1\n",
      "Name: count, dtype: int64\n",
      "Architectures Affected by Critical Services:\n",
      "              id                                           services\n",
      "0    WS2Qgx0qgCM       [EC2, EC2, CloudTrail, OpenSearch, STS, IAM]\n",
      "2    6LcSv9XocTY  [UserConsumerWeb, Connect, S3, S3, Transcribe,...\n",
      "9    ww5fiygF6eg  [UserCompanyDrone, ThirdParty, ThirdParty, Clo...\n",
      "19   c-1GXhOOOww  [UserConsumerMobile, UserConsumerAPI, S3, AppS...\n",
      "21   FftalZUxyiM  [S3, S3, Lambda, Lambda, DynamoDB, ApiGateway,...\n",
      "..           ...                                                ...\n",
      "374  StSuG6-iKW4  [UserConsumerEdge, IoTCore, Lambda, Lambda, St...\n",
      "383  EiljRL0977M  [EventBridge, Lambda, OpenSearch, EC2, Batch, ...\n",
      "385  DnTQ3matqts  [UserConsumerMobile, ApiGateway, ApiGateway, S...\n",
      "386  sKksbPPDznM  [UserConsumerMobile, ShieldAdvanced, WAF, Clou...\n",
      "391  ItpY_KKR94k  [EC2, MSK, Lambda, Fargate, Fargate, S3, OpenS...\n",
      "\n",
      "[105 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Identify most frequent proprietary AWS services\n",
    "service_counts = pd.Series([s for services in architecture_df['services'] for s in services]).value_counts()\n",
    "critical_services = service_counts[service_counts.index.isin([s for s, v in service_mapping.items() if v[\"Compatibility\"] != \"Fully Compatible\"])]\n",
    "print(\"Critical Proprietary Services:\")\n",
    "print(critical_services)\n",
    "\n",
    "# Analyze impact on architectures\n",
    "critical_architectures = architecture_df[architecture_df['services'].apply(lambda x: any(s in critical_services.index for s in x))]\n",
    "print(\"Architectures Affected by Critical Services:\")\n",
    "print(critical_architectures[['id', 'services']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id migration_strategy\n",
      "0    WS2Qgx0qgCM  Replace or Retain\n",
      "1    AzM_d7ZvzUE  Replace or Retain\n",
      "2    6LcSv9XocTY  Replace or Retain\n",
      "3    3yJZ6rPoZfg  Replace or Retain\n",
      "4    JRDGId6N49E  Replace or Retain\n",
      "..           ...                ...\n",
      "391  ItpY_KKR94k  Replace or Retain\n",
      "392  NfUwtK8ALtw  Replace or Retain\n",
      "393  0wnNlOg42dc  Replace or Retain\n",
      "394  chQ1phTqvnY  Replace or Retain\n",
      "395  aOZ4H98XROc  Replace or Retain\n",
      "\n",
      "[396 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate ease of migration for each architecture\n",
    "def assess_migration(services):\n",
    "    incompatible_services = [s for s in services if service_mapping.get(s, {}).get(\"Compatibility\") == \"Incompatible\"]\n",
    "    if incompatible_services:\n",
    "        return f\"Re-engineer ({len(incompatible_services)} services)\"\n",
    "    return \"Replace or Retain\"\n",
    "\n",
    "architecture_df['migration_strategy'] = architecture_df['services'].apply(assess_migration)\n",
    "print(architecture_df[['id', 'migration_strategy']])\n",
    "\n",
    "# Save migration analysis\n",
    "architecture_df.to_csv('output/migration_analysis.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
